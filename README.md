# CacheBlend (Under Construction): 

This is the code repo for [CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion](). The current implementation is based on [vLLM](https://github.com/vllm-project/vllm/tree/main).

The newest updates will always be at [LMCache](https://github.com/LMCache/LMCache). Stay tuned !!!
## Installation
To install CacheBlend depenencies
```
git clone git@github.com:YaoJiayi/CacheBlend.git
cd CacheBlend/vllm_blend
pip install -e .
cd ..
```


## Example run
### Run LLM inference with CacheBlend
```
python example/blend.py
```
## References
